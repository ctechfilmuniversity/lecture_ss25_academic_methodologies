name: inverse
layout: true
class: center, middle, inverse
---

# Academic Methodologies

### Prof. Dr. Lena Gieseke | l.gieseke@filmuniversitaet.de  

#### Film University Babelsberg KONRAD WOLF

---
layout: false

## Today

* Paper Check-In

--


* Experimental Studies  


???

* Variables
* Hypothesis Testing
* Control Conditions
* Sampling
* Significance Tests
* Grouping
* Reliability
* Design & Execution Steps

--
* Statistics in a Nutshell

---
## Your Paper

Scope
* Requirement: between 4-6 pages with the given template
* Estimated word counts: 3000 - 5000

???
* Single column with font size 12: 7-12 pages
* Show template in overleaf

--

Template
* [Linked in the conference script →](https://ctechfilmuniversity.github.io/lecture_ss25_academic_methodologies/02_scripts/am_01_conference_script.html#format)

--

<br />
Feedback Methodologies

---
template:inverse


# Quantitative Research


???
  
* There is an objective reality, which can be described and which we approach step by step or measure.


---
## Quantitative Research

Systematic investigation of observable phenomena via *statistical*, *mathematical*, or *computational techniques*.  

--

<br />
**The process of measurement is central.**

???
  

* Connection between empirical observation and mathematical expression of quantitative relationships
* In humanities / social sciences often understood as an *standardised approach*
    * Unification and generalisation of certain methods, e.g. conducting interviews

--

<br />
1. How to collect data?

--
2. How to analyse data?


???

Potential goal: To make conclusion from the collected data about the whole population.



---
## Quantitative Research

Systematic investigation of observable phenomena via *statistical*, *mathematical*, or *computational techniques*.  

<br />
**The process of measurement is central.**


<br />
1. **How to collect data?**
2. How to analyse data?


???

Can be part of a qualitative approach

* Survey answers
* Coding labels
* Case study analysis

Potential goal: To make conclusion from the collected data about the whole population.


---
template:inverse

# Experiments


---
## Experimental Research

With experiments researchers aim to measure *cause* and *effect*.

---

.center[<img src="../02_scripts/img/experiments/experiments_18.png" alt="experiments_18" style="width:44%;"> .imgref[[[[pinimg]]](https://i.pinimg.com/originals/6b/d5/97/6bd597153de701d4355979ed6647a941.jpg)]]





???
  

* Examples for cause and effect?

Cause and effect examples might look like:
  
---
## Experimental Research

* I flipped the light switch, so the light came on.
* As the wind speed increases, the sail boat moves faster.
* When the ocean is extremely polluted, coral reefs die.
* When I give you too much homework, you don't like me.



???
  

* Tsunamis happen when tectonic plates shift.
* I have been smoking cigarettes for ten years, so I got lung cancer.
* Maria didn't follow the recipe correctly, so the cake did not come out as expected.  
  
You get the idea.


*How can we detect such cause and effect relationships?*

Well, we can make *observations* of the world. 

Let's say we want to investigate whether a novel keyboard is easy to use. We have a number of participants use the keyboard for a while and ask them afterwards "Is the keyboard easy to use?" and have the following answers

---
## Experimental Research

*Is this keyboard easy to use?*

--

![experiments_01](../02_scripts/img/experiments/experiments_01.png)

--

*Does this proof that the keyboard is easy to use?*

---
## Experimental Research

Agreement because


???
How do we know that participants rated the keyboard easy to use because the keyboard was actually easy to use or because

--
* They want to support us in our research?

--
* They were impressed by the system’s novelty?

--
* They liked the colors of the system?

--
* They were in a really good mood because the German football team won the world cup yesterday and everything in the world is great?
* …

--

> Mere observation will not help us to find an valid answer!


???
  

* Or, imagine a researcher may observe that 8 out of 10 teenagers in a class who frequently play a specific computer game can touch type (typing without looking at the keyboard) while only 2 out of 12 teenagers in the same class who do not play the game can touch type. This is an interesting observation. But it does not allow the establishment of a relationship between the two factors: playing the game and typing. Neither does it enable the researcher to explain why this happens.
* For example, the researcher may collect data on the number of hours that the teenagers play the computer game per week and measure their typing speed. This will help to identify a relationship between multiple factors, e.g. playing the game and typing. This is called a relational investigation. Let's say the recorded numbers show that and the more a teenager plays, the more the typing speed increases.


---
## Experimental Research

Have a look at this table about stork populations and birthrates:

.center[<img src="../02_scripts/img/experiments/experiments_02.png" alt="experiments_02" style="width:33%;">  .imgref[[[brixtonhealth]](http://www.brixtonhealth.com/storksBabies.pdf)]]


---
## Experimental Research


.center[<img src="../02_scripts/img/experiments/experiments_03.png" alt="experiments_03" style="width:60%;">.imgref[[[brixtonhealth]](http://www.brixtonhealth.com/storksBabies.pdf)]]

The higher the number of stork breeding pairs, the higher the birth rate.

???
  

* Based on statistical operations (we will come back to this), one can claim (there is statistical significance) that the higher the number of stork breeding pairs, the higher the birth rate, hence there is a correlation between those variables. 


---
## Experimental Research

*Can we now claim that storks cause babies?*

<img src="../02_scripts/img/experiments/experiments_19.png" alt="experiments_19" style="width:44%;"> .imgref[[[livescience]](https://www.livescience.com/62807-why-storks-baby-myth.html)]

--

<br />
No, we have only established a correlation.


???
  
* Well, no, not all all! We have only established that there is a relationship between the numbers, but we know nothing about a *causation*. 
* How do we call this relationship?


## Correlation

.center[<img src="../02_scripts/img/experiments/experiments_06.png" alt="experiments_06" style="width:100%;">  [[mathsisfun]](https://www.mathsisfun.com/data/correlation.html)]


???
  

* A positive correlation indicates the extent to which those variables increase or decrease in parallel
* A negative correlation indicates the extent to which one variable increases as the other decreases
* [(Pearson’s) Correlation Coefficient](https://en.wikipedia.org/wiki/Pearson_correlation_coefficient) is a statistical measure of the *degree* to which changes to the value of one variable predict change to the value of another. This value is often called *r* and indicates *direction* and *strength* of a relationship between two variables. The closer it is to +1 or -1, the stronger there is a linear relationship between variables. 0 means there is no correlation, meaning no relationship.


---
## Experimental Research


Observation can tell us how things are **related**, not cause and effect.

--

<br />
In statistics *correlation* is any relationship between two random variables. 




???
  

* In statistics *correlation* (dependence or association) is any statistical relationship, whether causal or not, between two random variables. 

---
## Experimental Research


Explanation 1: Children cause storks.  
  
* For example, the crying of babies attract storks.
  
--
  
Explanation 2: Storks cause children.  
  
* For example, the myth is true and storks bring babies.
  
--
  
Explanation 3: A third unknown variable causes both.  
  
* For example, the village environment is more friendly to storks and families that desire children.
* In this case the number of storks has no impact on the number of babies or vice versa.


???
There are the following causation scenarios possible:  
  
Explanation 3 is also called a *[tertium quid](https://en.wikipedia.org/wiki/Tertium_quid)*, meaning that there is a an unidentified third element to a relationship of two knowns.



* With stating interpretations of such results, we have to be equally careful:
    * If I want more babies should I move to an area with many storks?
        * We can say for sure: No! Storks do not cause babies.
    * If I want more babies should I move to a rural area?
        * Well, even though rural areas are a proven cause for a higher birth rate there are still many more factors that influence whether you are having babies or not...
* That there is simply a correlation and no cause and effect, could be similar true for the example of playing a video game and typing capabilities. It might possible that teenagers who read well tend to type faster and that teenagers who read well tend to like the game more and spend more time on it. Here, the the relationship of values can be due to the hidden factor of reading (this is a bit far fetched - just to give you an example). Also, it might be possible that teenagers who type well tend to like the game more, as they already type well and because of that spend more time on it.
* A famous real-world example refers to a report, issued in 1964 from the United States’ Surgeon General, claiming that cigarette smoking causes lung cancer. Unfortunately, the evidence in the report was based primarily on correlations between cigarette smoking and lung cancer. The report came under attack not just by tobacco companies, but also by some prominent (probably smoking) statisticians. They claimed that there could be a hidden factor – maybe some kind of genetic factor – which caused both lung cancer and people to want to smoke (i.e., nicotine craving). If that was true, then while smoking and lung cancer would be correlated, the decision to smoke or not smoke would have no impact on whether you got lung cancer, a claim of great popularity at that time. [[6]](https://outsmartstatistics.weebly.com/examples--explanations.html)

---
## Experimental Research

In summary, *correlation does not imply causation*.  
  
<br />
It only shows a relationship between variables - which might be caused by any factor(s).


???
  
A correlation only proofs a relationship between the values of variables - which might be caused by any other factor or number of factors.

--
  
<br />
*So, what could we do to learn about cause and effect?*

---
## Experimental Research

> The only legitimate way to try to establish a causal connection statistically is through the use of randomized experiments.  
  
Utts, Jessica (2005) Seeing Through Statistics, Brooks/Cole (Thompson), p. 211.


???
  

* *How could such an experiment look like?*
* For example, for the study about video game and typing we, could assign the teenagers randomly to two groups. One group will spend a certain amount of time playing the computer game every week and the other group will not. After a period of time (e.g., 3 months or longer), we measure each teenager's typing speed. If the teenagers who play the computer game type significantly (and here we have to prove significance - we will come back to this) faster than the teenagers who do not play the game, we can draw conclusions that playing this computer game improves the typing skills of teenagers.

---
## Experimental Research

A well-designed, carefully analyzed experiment (or, better yet, series of experiments) isolates cause and effect. 

--
  
<img src="../02_scripts/img/experiments/experiments_07.png" alt="experiments_07" style="width:70%;">


---
## Experimental Research

A well-designed, carefully analyzed experiment (or, better yet, series of experiments) isolates cause and effect. 

<img src="../02_scripts/img/experiments/experiments_08.png" alt="experiments_08" style="width:70%;">


---
## Experimental Research

A well-designed, carefully analyzed experiment (or, better yet, series of experiments) isolates cause and effect. 
  
<img src="../02_scripts/img/experiments/experiments_09.png" alt="experiments_09" style="width:70%;">


???
  

* However, already keep in mind that the result of an experiment establishes a *likelihood of causality*. Interpretations are based on statistics and do not give evidence to a deterministic causation. They do not prove that *If this is done, then this will be the result in all cases.*  Instead, what they say is, *If this is done, under these circumstances, then on average this will be the result.*.


---
.header[Experimental Research]

## Variables

Experiments are based on *dependent* on *independent* variables.

<img src="../02_scripts/img/experiments/experiments_10.png" alt="experiments_10" style="width:70%;">


---
.header[Experimental Research]

## Variables

Experiments are based on *dependent* on *independent* variables.

<img src="../02_scripts/img/experiments/experiments_11.png" alt="experiments_11" style="width:70%;">


---
.header[Experimental Research]

## Independent Variable(s)

*What we change…*

--

*...meaning the cause of the effect.*  


???
  

* Independent variables refer to the factors that the researchers are interested in studying or the possible *cause* of the change. Hence, the independent variables describe the aspects that we change during an experiment, e.g. the hours the teenagers play the game in the previously mentioned example about playing a video game and typing capabilities. The term *independent* is used to suggest that the variable is independent of a participant's particular behavior or result.

--
  
<br />
How to manipulate a single aspect only?


???
  

* Here it is crucial to aim for manipulating a single aspect only, meaning in theory by keeping all other factors stable, such as environments and setups. In praxis, this is hard to archive, e.g. participants, meaning humans, are never identical.

--
* Keeping all other factors stable
* Randomizing non-controllable factors



???

* Environment, weather, intelligence, mood, …
* BUT, people, situations, … are never identical!


* Technology
    * different types of technology or devices, such as typing versus speech-based dictation, mouse versus joystick, touch pad, and other pointing devices; 
    * different types of design, such as pull-down menu versus pop-up menu, font sizes, contrast, background colors, and website architecture.
* Users
    * age, gender, computer experience, professional domain, education, culture, motivation, mood, and disabilities.
* Use of technologies
    * Both physical factors, such as environmental noise, lighting, temperature, vibration, users' status (e.g., seated, walking or jogging), and social factors, such as the number of people surrounding the user and their relation to the user.

---
.header[Experimental Research]

## Dependent Variable(s)

*What we measure…*

--

*...meaning the outcome or effect.*


???
  

* Dependent variables refer to the outcome or effect that the researchers are interested in. Hence, dependent variables describe the aspects that derive from the controlled change of the independent variable, e.g. speed of typing in the previously mentioned example.  
* The term *dependent* is used to suggest that the variable is dependent on a participant's particular behavior. 

--
  
<br />

Dependent variables are commonly referred to as *scores* and can be measured.


???
*  in different scales such as categorial (red, blue, green) vs. ratio scale data (time in ms)


Dependent variables frequently measured can be categorized into five groups: 

* efficiency,
* accuracy,
* subjective satisfaction,
* ease of learning and retention rate, and
* physical or cognitive demand.


* Efficiency describes how fast a task can be completed. Typical measures include time to complete a task and speed (e.g., words per minute, number of targets selected per minute).  
* Accuracy describes the states in which the system or the user makes errors.  
* Satisfaction experience. The data is normally collected using Likert scale ratings (e.g., numeric scales from 1 to 5) through questionnaires.  
* Ease of learning and retention rate describe how quickly and how easily an indi vidual can learn to use a new application or complete a new task and how long they retain the learned skills (Feng et al., 2005, as cited in [1]).
* Variables in the fifth category describe the cognitive and physical demand that an application or a task exerts on an individual or how long an individual can interact with an application without significant fatigue.

---
template:inverse

# Hypothesis Testing


---
.header[Experimental Research]

## Hypothesis

An experiment normally starts with a research hypothesis. 


???
  

* What is that?

--
  
<br />

A hypothesis is a precise problem statement that can be directly tested through an empirical investigation. 


???
  

* Compared with a theory, a hypothesis is a smaller, more focused statement that can be examined by a single experiment (Rosenthal and Rosnow, 2008, as cited in [1]).

In order to conduct a successful experiment, it is crucial to start with one or more good hypotheses (Durbin, 2004, as cited in [1]).

--
  
<br />
Then, the experiment can **accept or reject the hypothesis**. 


???
  

* There is no limit on the number of hypotheses that can be investigated in one experiment. However, it is generally recommended that researchers should not attempt to study too many hypotheses in a single experiment. Normally, the more hypotheses to be tested, the more factors that need to be controlled and the more variables that need to be measured. This results in very complicated experiments, subject to a higher risk of design flaws.

---
.header[Experimental Research]

## Hypothesis Testing

Formalized statistical technique aiming to give a likelihood of a hypotheses to be true - often for the whole population. 

---
.header[Experimental Research]

## Hypothesis Testing

Based on a ***null-hypothesis***.

--

<br />
Assumes that there is **no** effect of the change in the independent variable on the measured variable.  

--

<br />

We need to falsify the null-hypothesis!

--

1. Proof that there is a effect
  
--
  
2. Proof that the effect is not random

???

Then the experiment aims to *disprove* the null-hypothesis using statistical measures. These statistical tests also aim to prove that the perceived effect is not random.


## Hypothesis Testing

Null hypothesis H<sub>0</sub>

* Assumes that there is no difference between two values (e.g. the means of the different experiment groups)
* H<sub>0</sub>: 𝜇1 = 𝜇2

Alternative hypothesis H<sub>A</sub> (or also often called H<sub>1</sub> )

* Assumes significant differences
* H<sub>A</sub>: 𝜇1 != 𝜇2 or 𝜇1 > 𝜇2 or 𝜇1 < 𝜇2

---
.header[Experimental Research]

## Hypothesis Testing

A helpful example is here the criminal trial analogy:

--

* The defendant is innocent until proven guilty
    * H<sub>0</sub>: Defendant is not guilty
    * H<sub>A</sub>: Defendant is guilty
* There is no effect until proven otherwise

--

> In statistics, we always assume the null hypothesis is true, meaning that there is no cause and effect - until proven otherwise. Data is the evidence.


???
  

* As another example, suppose the developers of a website are trying to figure out whether to use a pull-down menu or a pop-up menu in the home page of the website. For this research case, the null and alternative hypotheses can be stated in classical statistical terms as follows:
    * H<sub>A</sub>: There is no difference between the pull-down menu and the pop-up menu in the time spent locating pages.
    * H<sub>A</sub>: There is a difference between the pull-down menu and the pop-up menu in the time spent locating pages.
* This might feel foreign to you. We will come back to this in the chapter about statistics.




---
.header[Experimental Research]

## Experiment Design


???
  

* The basic structure of an experiment can be determined by answering two questions:

--

* How many independent variables?
* How many different values does each independent variable have?


???
  

The answer to the first question determines whether we need a *basic* design or a *factorial* design. If there is one independent variable, we need only a basic one-level design. If there are two or more independent variables, factorial design is the way to go. The answer to the second question determines the number of conditions needed in the experiment. 

![experiments_13](../02_scripts/img/experiments/experiments_13.png)  

[1]

---
.header[Experimental Research | Basic Structure]

## A Single Independent Variable


???
  

* When we study a single independent variable, the design of the experiment is simpler than cases in which multiple variables are involved. The following hypotheses all lead to experiments that investigate a single independent variable:

--

H<sub>0</sub>: There is no difference in typing speed when using a QWERTY keyboard, a DVORAK keyboard, or an alphabetically ordered keyboard.


???
  

* H<sub>2</sub>: There is no difference in the time required to locate an item in an online store between novice users and experienced users.
* H<sub>3</sub>: There is no difference in the perceived trust toward an online agent among customers who are from the United States, Russia, China, and Nigeria.
  
--

* Independent variable: the keyboard
* Dependent variable: typing speed

--
  
Three conditions:

* QWERTY keyboard
* DVORAK keyboard
* alphabetically ordered keyboard

???
  

* The number of conditions in each experiment is determined by the possible values of the independent variable. 

The experiment conducted to investigate hypothesis H<sub>2</sub> would involve two conditions:

* novice users and
* experienced users.

And the experiment conducted to investigate hypothesis H<sub>3</sub> would involve four conditions: customers from

* the United States,
* Russia,
* China, and
* Nigeria.

Once the conditions are set, we need to determine the number of conditions to which we would allow each participant to be exposed to by selecting either a grouping (see the grouping section).

[1]

---
.header[Experimental Research | Basic Structure]

## Multiple Independent Variables


???
  

* Factorial designs are widely adopted when an experiment investigates more than one independent variable. Using this method, we divide the experiment groups or conditions into multiple subsets according to the independent variables. It allows us to simultaneously investigate the impact of all independent variables as well as the interaction effects between multiple variables. The number of conditions in a factorial design is determined by the total number of independent variables and the level of each independent variable. 
* As an example consider again to run an experiment to compare the typing speed when using three types of keyboard (QWERTY, DVORAK, and Alphabetic). 
* Now we are also interested in examining the effect of different tasks, e.g. composition vs transcription, on the typing speed. 

Two independent variables can be investigated in one experiment: 

--

1. Type of keyboard
2. Type of task


???
* H<sub>0</sub>: There is no difference in typing speed when using a QWERTY keyboard, a DVORAK keyboard, or an alphabetically ordered keyboard.


---
.header[Experimental Research | Basic Structure]

## Multiple Independent Variables

The variable *type of keyboards* has three conditions:

* QWERTY
* DVORAK
* Alphabetic


--

The variable *type of tasks* has two conditions:

* Transcription
* Composition

--



`Number of conditions = 3 * 2 = 6`

???
  

* Therefore, the total number of conditions in this experiment is calculated according to the following equation: 



|               | QWERT | DVORAK | Alphabetic |
| ------------- | ----- | ------ | ---------- |
| Composition   | C1    | C2     | C3         |
| Transcription | C4    | C5     | C6         |


???
  

* In the first three conditions, the participants would all complete composition tasks using different kinds of keyboard. In the other three conditions, the participants would all complete transcription tasks using different keyboards. When analyzing the data, we can compare conditions in the same row to examine the impact of keyboards. The effect of the tasks can be examined through comparing conditions in the same column. As a result, the effect of both independent variables can be examined simultaneously through a single experiment.

As another example, consider designing an experiment to investigate the following hypothesis:

> There is no difference between the target selection speed when using a mouse, a joystick, or a trackball to select icons of different sizes (small, medium, and large). 

There are two independent variables in this hypothesis: the type of pointing device and the size of icon. Three different pointing devices will be examined: a mouse, a joystick, and a trackball, suggesting three conditions under this independent variable. Three different target sizes will be examined: small, medium, and large, suggesting three conditions under this independent variable as well. Since we need to test each combination of values of the two independent variables, combining the two independent variables results in a total of nine (3×3=9) conditions in the experiment. The identification of dependent variables will allow us to further consider the appropriate metric for measuring the dependent variables, such as speed, number of errors. Here, we need to consider the objective of the experiment to determine which dependent variable and which measure is more appropriate.

[1]

---
.header[Experimental Research]

## Grouping

*Which participants should do tasks under which condition?*

???
  

* This is a critical step in experimental design and the decision made has a direct impact on the quality of the data collected as well as the statistical methods that should be used to analyze the data.

---
.header[Experimental Research | Grouping]

## Between-Group Design

Also called *independent measure* design, and participants are assigned to *one* condition only:

.center[<img src="../02_scripts/img/experiments/experiments_14.png" alt="experiments_14" style="width:36%;">]



???
  

* For example:

![experiments_15](../02_scripts/img/experiments/experiments_15.png)  
[[1]]

Advantages of this grouping are the simplicity of the setups, that there is less chance of learning or fatigue effects and that is is sometimes practically impossible for an individual to participate in all conditions, e.g. due to time constraints. Also from a statistical perspective, between-group design is a cleaner design.

Disadvantages are that between-groupings are usually more laborious and costly as you need a higher number of participants and therefore more time, effort, etc. Also, overall, this setup is less robust and more prone to outliers as individual differences are more influential. This adds to the need of a comparatively larger number of participants under each condition.

For example, if an experiment has 4 conditions and 16 participants are needed under each condition, the total number of participants needed is 64.

[1]

---
.header[Experimental Research | Grouping]

## Within-Group Design

Also called *repeated-measure* design, participants are assigned to *all* conditions. 

.center[<img src="../02_scripts/img/experiments/experiments_16.png" alt="experiments_16" style="width:36%;">]

--
Requires a randomized order of tasks.

???
  

* For example:

![experiments_17](../02_scripts/img/experiments/experiments_17.png)  
[[1]]

Advantages of this grouping are that it is more economic and individual differences are canceled out better. If we change the design of the experiment with 4 conditions and 16 participants from a between-group design into a within-group design, the total number of participants needed would be 16, rather than 64.

A disadvantage is that within-groupings are prone to side-effects. E.g. previous conditions might influence the behavior of participants in the following conditions, e.g. with learning or fatigue effects. Since the participants complete the same types of task under multiple conditions, they are very likely to learn from the experience and may get better in completing the tasks. Similarly, participants may get tired or bored during the process.


## Grouping

| Between-Group Design                                                | Within-Group Design                             |
| ------------------------------------------------------------------- | ----------------------------------------------- |
| + Cleaner                                                           | + Smaller sample size                           |
| + Better control of confounding such as fatigue and learning effect | + Effective isolation of individual differences |
| - Large sample size                                                 | - Hard to control learning effects              |
| - Large impact of individual differences                            | - Large impact of fatigue                       |
| - Harder to get statistically significant results                   |                                                 |

[1]


???
  

Generally speaking, between-group design should be adopted when the experiment investigates

* simple tasks with limited individual differences
* tasks that would be greatly influenced by the learning effect or
* problems that cannot be investigated through a within-group design.
  
After choosing a between-group design for an experiment, we need to take special caution to control potential confounding factors. Participants should be randomly assigned to different conditions whenever possible. When assigning participants, we need to try our best to counterbalance potential confounding factors, such as gender, age, computing experience, and internet experience, across conditions. In other words, we need to make sure that the groups are as similar as possible, except for the personal characteristics that are experimental variables under investigation.

Within-group design is more appropriate when the experiment investigates

* tasks with large individual differences,
* tasks that are less susceptible to the learning effect, or
* when the target participant pool is very small.
  
Having decided to adopt a within-group design, you need to consider how to control the negative impact of learning effects, fatigue, and other potential problems associated with a within-group design.  

An effective approach to reduce the impact of the learning effect is for example to provide sufficient time for training, which reduces the learning effect during the actual task sessions. To address the problem of fatigue caused by multiple experimental tasks, we need to design experiment tasks frugally, reducing the required number of tasks and shortening the experiment time whenever possible. It is generally suggested that the appropriate length of a single experiment session should be 60 to 90 minutes or shorter (Nielsen, 2005, as cited in [1]).
  
[1]


---
.header[Experimental Research | Hypothesis Testing]

## Control Conditions

--

Experiments are often based on *control conditions*.

--

Compare two scenarios:

1. The cause is present (the *experimental condition*)
2. The cause is absent (the *control condition*)

???
  
The idea is to compare two situations where in one the supposed cause is present (the *experimental condition*) to one where it is absent (the *control condition*).
   
The control condition isolates the suspected effect on the dependent variable. 

--
  
Null-hypothesis: There is no difference between Scenario 1 and 2.


???

* For example, let's talk about storks and babies again. Let's say we are still considering that storks do, in fact, cause babies.

*How could the experimental and controls conditions look like to test our hypothesis?*

* The experimental condition is a group of couples residing on a stork farm.
* The control condition are a group of couples residing on a chicken farm.
    * Extra-long artificial beaks would have to be fitted to the chickens and they would need to wear red stilts to make the subjects *blind* to their respective group.

Hypothesis testing

* H<sub>0</sub>: There is no effect of living on a stork farm on the birthrate.
* H<sub>A</sub>: There is an effect of living on a stork farm on the birthrate.

---
.header[Experimental Research]

## Sampling

--

*Which specific entities to test?*


???
  

* In a true experimental design, the researcher can fully control or manipulate the experimental conditions so that a direct comparison can be made between two or more conditions while other factors are, ideally, kept the same. One aspect of the full control of factors is complete *randomization*, which means that the researcher can randomly assign participants to different conditions. The capability to effectively control for variables not of interest, therefore limiting the effects to the variables being studied, is the feature that most differentiates experimental research.

--

Sampling decides how to select individuals from a population.

--
  
<br />

> Correct sampling is crucial to any type of study and an incorrect sample frame can destroy a study, regardless of the sample size.


???
  

* A famous example is the president election in the US in 1936. The candidates were the Republican Landon vs. the Democrat Roosevelt. Before the election there was a telephone survey by „Literary Digest“ with as many as 10,000,000 phone calls and
2,300,000 participants (45,600,000 voters). The prediction was clear, there would be a landslide victory for Landon.

However, the election results turned out to be as follows (red republican, blue democratic wins)

---
.header[Experimental Research]

## Sampling

.center[<img src="../02_scripts/img/experiments/sampling_04.png" alt="sampling_04" style="width:100%;">]

???
  

* A famous example is the president election in the US in 1936. The candidates were the Republican Landon vs. the Democrat Roosevelt. Before the election there was a telephone survey by „Literary Digest“ with as many as 10,000,000 phone calls and
2,300,000 participants (45,600,000 voters). The prediction was clear, there would be a landslide victory for Landon.

However, the election results turned out to be as follows (red republican, blue democratic wins)

--

*Why did the survey go so wrong?*


???
  

* Think about the year 1936 and who would own a telephone at that time...

---
.header[Experimental Research | Sampling]

## Randomization

Randomize all factors that you can not fully control and might influence the effect.

* Participant
* Order of tasks


???
  
> A simple random sample (SRS) of size n consists of n individuals from the population chosen in such a way that every set of n individuals has an equal chance to be the sample actually selected.  
  
Moore, David S. and George P. McCabe (2006), Introduction to the Practice of Statistics, fifth edition, Freeman, p. 219

In a well-designed experiment, you try to randomize all factors possible, such as the assignment of participants and the order of tasks. 

* For example, you assign participants randomly into groups in order to spread factors such as intelligence, motivtation, tiredness, physical capabilities and such. Similarly, you want to run conditions and tasks in random order to avoid sequence effects, such as learning or training influences or tiredness for the last tasks.
* Nowadays, software-driven randomization is commonly used for tasks like this. A large number of randomization software resources are available online, such as https://www.randomizer.org/. Randomization functions are also available in most of the commercial statistical software packages.  


<img src="../02_scripts/img/experiments/sampling_01.png" alt="sampling_01" style="width:100%;">  
[[researchhubs]](researchhubs.com/post/ai/data-analysis-and-statistical-inference/observational-studies-and-experiments-sampling-and-source-bias.html)


### Stratified Sampling

<img src="../02_scripts/img/experiments/sampling_02.png" alt="sampling_02" style="width:100%;">  
[[researchhubs]](researchhubs.com/post/ai/data-analysis-and-statistical-inference/observational-studies-and-experiments-sampling-and-source-bias.html)


* When subpopulations within an overall population vary, it could be advantageous to sample each subpopulation (stratum) or group independently in order to reduce sample variation. For this the population is divided into homogeneous groups (strata), and then a sample is randomly drawn from each strata. For example the study population is divided into 3 groups based on age and then you take a random sample from each group. 


### Cluster Sampling 

<img src="../02_scripts/img/experiments/sampling_03.png" alt="sampling_03" style="width:100%;">  
[[researchhubs]](researchhubs.com/post/ai/data-analysis-and-statistical-inference/observational-studies-and-experiments-sampling-and-source-bias.html)


* This is also known as block sampling. Here, the population is first divided into subgroups of a certain meaning and you randomly select members from some of the clusters. For example, the study population is divided into 5 clusters based on the hospital in which they received treatment. Then, only two of the clusters are randomly chosen to be included in the sample.



*  As you cannot test the whole population, you chose some people from the population as sample and assume that these samples reflect the whole population.

--

You must report your sampling process and experiment setup in detail.

???
  

When writing about your research, keep in mind that you have to report on the formal aspects of your sampling process, such as

* Average age and gender split
* Recruitment process
* Participants’ background
* Aspect that are relevant for specific study such as eyesight, colorblindness for example


* There are no fixed rules in place on how many participants you need for a valid experiment. The number depends on the project and the goals and your the set-up. As a rule of thumb you should plan with at least 10 participants. Depending on your chosen evaluation techniques there also might be statistical guidelines on the required number of participants.
* Of course, if there is a specific user group for your task you need to choose participants accordingly. Sometimes, it makes life much easier to limit a research task to a certain group. Then your results are less generalizable but at least you get things done. As a rule of thumb, other people on the team are usually NOT AT ALL representative (however it is common practice in CS research to recruit department members as study participants...). 

There are countless way to recruit participants such as in specific forums or students from lectures that fit topic-wise. Another options in modern times is Amazon turk. From their [website](https://www.mturk.com/):

> Amazon Mechanical Turk (MTurk) is a crowdsourcing marketplace that makes it easier for individuals and businesses to outsource their processes and jobs to a distributed workforce who can perform these tasks virtually. This could include anything from conducting simple data validation and research to more subjective tasks like survey participation, content moderation, and more. MTurk enables companies to harness the collective intelligence, skills, and insights from a global workforce to streamline business processes, augment data collection and analysis, and accelerate machine learning development.

![mturk](../02_scripts/img/experiments/mturk.png)  
[[mturk]](https://www.mturk.com/)

The validity of research conducted with the Mechanical Turk worker pool has been questioned in multiple research communities. Problems address include, for example:

* Amazon doesn't share the method of selecting participants with researchers.
* Demographics of the pool of participants are not known.
* It is unclear if Mechanical Turk uses fiscal, political, or educational limiters in their selection process.
* It is ‘bought' research.

In 2013, for example, the Siggraph paper 

Sean Bell, Paul Upchurch, Noah Snavely, Kavita Bala. [OpenSurfaces: A Richly Annotated Catalog of Surface Appearance](http://opensurfaces.cs.cornell.edu/). ACM Transactions on Graphics (SIGGRAPH 2013).

was criticized, as it is mainly a database of photographs with different surface material characteristics segmented and tagged in each image, which was done with mechanical turk tasks. But if you assume the minimal cost of $0.01 per task and the image database included 385,362 assigned tasks, this projected did cost less than $4000, which is not that much if you consider what certain hardware might costs, for example. If a database is enough contribution for a research paper is up to the reviewers to decided, which they did. So, my humble opinion is that I don't see much of a problem here, especially because I think such databases are of great value to a research community.

However, if the mechanical turk workers are treated and paid fairly is a different story. This questions of for example investigated in the project [The Laboring Self](https://www.utdallas.edu/magazine/5984/professors-collaboration-highlights-plight-of-invisible-workers/), which is
> a participatory visual arts project that drew parallels between unregulated labor and its effect on workers' bodies in the Industrial Revolution and Amazon.com's Mechanical Turk online job platform.


## Informed Consent


* When working with humans, you are usually required to collect an informed consent formally. The standards differ here from discipline to discipline and from country to country. E.g. at an north american university all studies including humans usually must be officially registered and approved from the ethics committee of that university. In Germany it is less strict.


A consent form is a must-have if personally identifiable information is collected.


* which is information that can

* uniquely identify, contact, or locate a single person, or
* uniquely identify a single individual with other sources.


Informed consent implies to inform the participants about

* The background of the experiment
    * Context in which the research takes place?
    * What do participants have to expect?
* The form of data collection applied
    * What kind of data will be logged?
    * Who has access and how will it be secured?
    * What is going to be reported?
* Information participants that
    * they do not have to e.g. answer questions if they don’t want to, and
    * they can cancel the experiment at any time without explanation.


.center[<img src="../02_scripts/img/experiments/consent_01.png" alt="consent_01" style="width:100%;">]


Further ethical considerations when working with human refer to aspects of

* Deception
    * Consider whether it is ethical to not inform participants about the real context of the experiment
* Debriefing
    * After the experiment, answer the participants questions
* Confidentially
    * Keep information confidential at any time!
    * Make sure that your system is sufficiently secured.
* Do not physically or psychologically harm your participants and protect them from any harm


## Significance Tests


???
  

* Almost all experimental investigations are analyzed and reported through *significance* tests.
* These are statistical test to which we will come back in the chapter about statistics. But as this aspect is crucial to understand for the use and interpretation of experiments, I am already introducing the general concept of significance tests here.


In study reports and their publications you will read something like

* On average, participants performed significantly better (F(1,25) = 20.83, p < 0.01) ...
* A t-test showed that there was a significant difference in ... (t(11) = 6.28, p < 0.001) ...


*Why do you need to run significance tests on your data?*


???
  

* What is for example wrong with comparing two mean values of error rate and then claiming that the application with the lower mean value is more accurate than the other application?



## Significance Tests

Consider the following statement:  

*The average height of three males (Mike, John, and Ted) is 165cm. The average height of three females (Mary, Rose, and Jessica) is 178cm. So females are taller than males.*


???
  

* It should not be difficult for you to tell that the first statement is correct while the second one is not. In the first statement, the targets being compared are the heights of two individuals, both known numbers. Based on the two numbers, we know that Mike is taller than Mary. When the values of the members of the comparison groups are all known, you can directly compare them and draw a conclusion. No significance test is needed since there is no uncertainty involved. 
* So, what is wrong with the second statement? The claim that females are taller than males is wrong due to inappropriate sampling. 


The distribution of the heights of the human population (and many other things in our life) follows a pattern called *normal distribution*. Data sets that follow normal distribution can be illustrated by a bell-shaped curve, with the majority of the data points falling in the central area surrounding the mean of the population (μ). The further a value is from the population mean, the fewer data points would fall in the area around that value. 

![significance_01](../02_scripts/img/experiments/significance_01.png) [1]

When you compare two large populations, such as males and females, there is no way to collect the data from every individual in the population. Therefore, you select a smaller group from the large population and use that smaller group to represent the entire population. As you know by now, this process is called sampling. In the situation described in statement 2 above, the three males selected as the sample population happened to be shorter than average males, while the three females selected as samples happened to be taller than average females, thus resulting in a misleading conclusion. Randomization methods and large sample sizes can greatly reduce the possibility of making this kind of error in research. Since we are not able to measure the heights of all males and females, we can only sample a subgroup of people from the entire population. 


Significance tests allow us to determine how confident we are that the results observed from the sampling population can be generalized to the entire population. 


For example, a *t*-test that is significant at *P < 0.05* suggests that we are confident that 95% of the time the test result correctly applies to the entire population. 



## Type I and Type II Errors


???
  

* In technical terms, significance testing is a process in which a null hypothesis (H<sub>0</sub>) is contrasted with an alternative hypothesis (H<sub>A</sub>) to determine the likelihood that the null hypothesis is true.
* All significance tests are subject to the risk of Type I and Type II errors. 
* A Type I error (also called an α error or a *false positive*) refers to the mistake of rejecting the null hypothesis when it is true and should not be rejected. 
* A Type II error (also called a β error or a *false negative*) refers to the mistake of not rejecting the null hypothesis when it is false and should be rejected (Rosenthal and Rosnow, 2008, as cited in [1]).



.center[<img src="../02_scripts/img/experiments/significance_02.png" alt="" style="width:80%;">  [[flowingdata]](https://flowingdata.com/2014/05/09/type-i-and-ii-errors-simplified/)]


???
  

![significance_03](../02_scripts/img/experiments/significance_03.png)  

* The criminal trail analogy of [Hypothesis Testing](#hypothesis-testing) is once again helpful here to understand the problem. Of course, in the ideal case, a trail should always reach the decision that the defendant is guilty when he is actually guilty and vice versa.
* But in reality, mistakes happen occasionally. Each type of error has costs. When a Type I error occurs, an innocent person would be sent to prison or in the US that person may even lose his or her life; when a Type II error occurs, a criminal is set free and may commit another crime. Which error are we more willing to accept as a society?

It is generally believed that Type I errors are worse than Type II errors. Statisticians call Type I errors a mistake that involves *gullibility* (*Leichtgläubigkeit*). A Type I error may result in a condition worse than the current state. For example, if a new medication is mistakenly found to be more effective than the medication that patients are currently taking, the patients may switch to new medication that is less effective than their current treatment. Type II errors are mistakes that involve *blindness* and can cost the opportunity to improve the current state. In the medication example, a Type II error means the test does not reveal that the new medication is more effective than the existing treatment; the patients stick with the existing treatment and miss the opportunity of a better treatment.

[1]

## Experiment Design


???
  

* Now we have discussed all basic components needed to put together an experiment.  


An experiment with multiple conditions and random assignments of participants and tasks is considered a *true* experiments. 


???
  

* Quasi-experiments refer to experiments with non-random assignments.


True experiments possess the following characteristics:

* A true experiment is based on at least one testable research hypothesis and aims to validate it.


* There are usually at least two conditions (a treatment condition and a control condition) or groups (a treatment group and a control group).

* The dependent variables are normally measured through quantitative measurements.


* The results are analyzed through various statistical significance tests.


* A true experiment should be designed and conducted with the goal of removing potential biases.

* A true experiment should be replicable with different participant samples, at different times, in different locations, and by different experimenters.


---
.header[Experimental Research]

## Reliability

--

Experiments should be replicable and then lead to the same results.


???
 

All research tasks should always strive for high reliability, meaning that the tasks can be replicated by other teams, in other locations, etc.

 

* Measurements of human behavior and social interaction however, are normally subject to higher fluctuations and, therefore, are less replicable in contrast to the hard sciences, such as physics, chemistry, and biology.
Fluctuations in experimental results are referred to as *errors*.

---
.header[Experimental Research]

## Reliability
  
Reflect on

--
  


* Random Errors

???
  

* Random errors are also called *noise*. They occur by chance and are not correlated with the actual value. There is no way to eliminate or control random errors but we can reduce the impact of random errors by enlarging the observed sample size. When a sample size is small, the random errors may have significant impact on the observed mean and the observed mean may be far from the actual value. When a sample size is large enough, the random errors should offset each other and the observed mean should be very close to the actual value.

--
  
* Systematic Errors

???
  

* To disproportionately weight in favor of or against an idea or thing is called [bias](https://en.wikipedia.org/wiki/Bias).  

Systematic errors are also called *biases* and they are completely different in nature from random errors. While random errors cause variations in observed values in both directions around the actual value, systematic errors always push the observed values in the same direction. As a result, systematic errors never offset each other in the way that random errors do and they cause the observed mean to be either too high or too low.

Systematic errors can greatly reduce the reliability of experimental results. They are the true enemy of experimental research. We can counter systematic errors in two stages: we should try to eliminate or *control biases* during the experiment when biases are inevitable, and we need to *isolate the impact* of them from the main effect when analyzing the data. 

There are five major sources of systematic errors:

--
    * Measurement instruments
    * Experimental procedures
    * Participants
    * Experimenter behavior
    * Experimental environment


???
  

* Bias Caused by Measurement Instruments
    * When the measurement instruments used are not appropriate, not accurate, or not configured correctly, they may introduce systematic errors. For example when the stop button of a timer is broken and it takes a moment to stop the time, we have a systematic addition of time to the actual task. If now another research team would reproduce the experiment, they would get different timings with a properly working stop watch.
* Bias Caused by Experimental Procedures
    * Inappropriate or unclear experimental procedures may introduce biases. As discussed previously, if the order of task conditions is not randomized in an experiment with a within-group design, the observed results will be subject to the impact of the learning effect and fatigue.  
    * Also, the instructions that participants receive play a crucial role in an experiment and the wording of the experiment instructions should be carefully scrutinized before a study. Slightly different wording in instructions may lead to different participant responses. In a reported HCI study (Wallace et al., 1993, as cited in [1]), participants were instructed to complete the task “as quickly as possible” under one condition. Under the other condition, participants were instructed to “take your time, there is no rush.” Interestingly, participants working under the no-time-stress condition completed the tasks faster than those under the time-stress condition. This suggests the importance and complexity of finding a suitable wording in instructions. It also implies that the instructions that participants receive must be highly consistent.
    * One approach to avoid biases attributed to experimental procedures, are *pilot studies*. A pilot study is a test run of the experiment and are critical for experiments to identify potential biases. No matter how well you think you have planned the study, there are always things that you overlook. A pilot study is the only chance you have to fix your mistakes before you run the main study.
* Bias Caused by Participants (Sampling)
    * Bias in sampling is sometimes called *ascertainment bias* (especially in biological fields). E.g. the above mentioned telephone survey about the presidential election in 1936 systematically favored rich peoples' opinions as at that time only rich people could be reached by telephone.
    * Bias in the selection of participants is often due to *convenience*, e.g. selecting members of the same department, students from the university, etc. For example, an analysis of leading psychology journals in the US found out that a random American undergraduate is about 4,000 times more likely than an average human being to be the subject of national academic study.
    * Make sure to recruit carefully and make sure that the participant pool is representative of the target population.
* Bias Due to Experimenter Behavior
    * Experimenter behavior is one of the major sources of bias. Experimenters may intentionally or unintentionally influence the experiment results. Any intentional action to influence participants' performance or preference is unethical in research and should be strictly avoided. However, experimenters may unknowingly influence the observed data. Spoken language, body language, and facial expressions frequently serve as triggers for bias. Just imagine an experimenter is introducing an interface to a participant. Then experimenter says, “Now you get to the pull-down menus. I think you will really like them.… I designed them myself!”. Or consider how it might influence a performance, if the experimenter arrives late and the participant had been waiting for 45 minutes...
* Bias Due to Environmental Factors
    * Environmental factors can be categorized into two groups: physical environmental factors and social environmental factors. Examples of physical environmental factors include noise, temperature, lighting, vibration, and humidity. Examples of social environmental factors include the number of people in the surrounding environment and the relationship between those people and the participant.

---
.header[Experimental Research]

## Reliability

All in all, how to overcome bias depends on your experiment. 


???

You have to think about what you are doing and what the, maybe invisible on first sight, effects of your methods are.

--

> […], bias cannot be assessed without external knowledge of the world.  
  
Herbert I. Weisberg (2010), Bias and Causation: Models and Judgment for Valid Comparisons, p. 26


???
  

* However, to overcome bias e.g. in experimental procedures and experimenter behavior, you should always have a checklist with all the steps of an experiment, starting with the welcoming words, ending with saying goodbye. Then you must use that checklist every single run and check off every single step.

--

> No matter how hard we try to avoid biases, they can never be completely eliminated. 


???
  

* Therefore, we should be careful when reporting the findings, even when the study results are statistically significant.

---
.header[Experimental Research]

## In Summary


???
  

* In summary, the steps to set up an experiment are:

--
.left-even[
* Identify a research hypothesis
]

---
.header[Experimental Research]

## In Summary

.left-even[
* Identify a research hypothesis
* Specify the design of the study
]

---
.header[Experimental Research]

## In Summary

.left-even[
* Identify a research hypothesis
* Specify the design of the study
]

.right-even[
* Independent Variable
    * The suspected cause
    * Isolate through randomization
* Dependent Variable
    * The suspected effect
    * Isolate through a control condition
]

---
.header[Experimental Research]

## In Summary

.left-even[
* Identify a research hypothesis
* Specify the design of the study
* Method to use
]

---
.header[Experimental Research]

## In Summary

.left-even[
* Identify a research hypothesis
* Specify the design of the study
* Method to use
]

.right-even[

<br /><br />
* Between-groups / within-groups
* Sampling
]

---
.header[Experimental Research]

## In Summary

.left-even[
* Identify a research hypothesis
* Specify the design of the study
* Method to use
* Plan the statistical analysis
]




---
.header[Experimental Research]

## In Summary

Execution

* Run a pilot study 
* Recruit participants
* Data collection sessions
* Analyze the data
* Report the results


???

* to test the design, the system, and the study instruments
* Recruit participants
    * Are they representative for the experiment?
    * Is the group large enough?
  

## Design & Execution Steps

Within a specific experiment session, go through the following steps:

1. Ensure that the systems or devices being evaluated are functioning properly, the related instruments are ready for the experiment.
2. Greet the participants.
3. Introduce the purpose of the study and the procedures.
4. Get the consent of the participants.
5. Assign the participants to a specific experimental condition according to the predefined randomization method.
6. Participants complete training tasks.
7. Participants complete actual tasks.
8. Participants answer questionnaires (if any).
9. Debriefing session.
10. Payment (if any).


Some words of advice based on my experiences

* Do not underestimate the time it takes to complete an experiment and the amount of planning involved. The administrative side of working with participants, e.g. when scheduling their time slots, etc. is an overhead to your research of which you must be sure that it is worth it. Never do an experiment under time pressure, e.g. two weeks before the paper deadline.
* Plan in enough time for each participant to ask questions after they have completed their tasks. In my experience they always have plenty of questions and it is simply rude not to take the time to answer them due to a too tight scheduling.
* There is no way around the maths (this is what reviewers check first) and planning the statistical analysis must be part of the design of the study *before* executing the experiment. Collecting data and then thinking about what to do with the data is doomed to fail (I have been there and it caused me a couple of sleepless nights because my experiment's data didn't fit into any standard statistical tests...).

---

## In Summary

Experiments help us to answer questions and identify *causal* relationships.  

--

<br />
> Successful experimental research depends on well-defined research *hypotheses* that specify the *dependent variables to be observed and measured* and the *independent variables to be controlled*. 

???
  

* Usually a pair of null and alternative hypotheses is proposed and the goal of the experiment is to test whether the null hypothesis can be rejected or the alternative hypothesis can be accepted. Good research hypotheses should have a reasonable scope that can be tested within an experiment; clearly defined independent variables that can be strictly controlled; and clearly defined dependent variables that can be accurately measured.  

--

<br />
*Significance testing* allows us to judge whether the results might be random.


???
  

* All significance tests are subject to two types of error. *Type I errors* refer to the situation in which the null hypothesis is mistakenly rejected when it is actually true. *Type II errors* refer to the situation of not rejecting the null hypothesis when it is actually false. It is generally believed that Type I errors are worse than Type II errors, meaning it is worse to accept an causal relationship when there is none.

Hence, the *design of an experiment* starts with a clearly defined, testable research hypothesis. During the design process, we need to answer the following questions:

* How many dependent variables are investigated in the experiment and how are they measured?
* How many independent variables are investigated in the experiment and how are they controlled?
* How many conditions are involved in the experiment?
* Which grouping to use, a between-grouping or within-grouping?
* What potential bias may occur and how can we avoid or control those biases?
  
All experiments strive for clean, accurate, and unbiased results. In reality, experiment results are highly susceptible to bias. Biases can be attributed to five major sources: the measurement instruments, the experiment procedure, the participants, the experimenters, and the physical and social environment. We should try to avoid or control biases through accurate and appropriate measurement devices and scales; clearly defined and detailed experimental procedures; carefully recruited participants; well-trained, professional, and unbiased experimenters; and well-controlled environments.

With its notable strengths, experimental research also has notable limitations when applied in fields such as HCI or CTech: difficulty in identifying a testable hypothesis, difficulty in controlling potential confounding factors, and changes in observed behavior as compared to behavior in a more realistic setting. Therefore, experimental research methods should only be adopted when appropriate.

## Limitations of Experimental Research

To date, experimental research remains one of the most effective approaches to making findings that can be generalized to larger populations. On the other hand, experimental research also has notable limitations.  

It requires well-defined, testable hypotheses that consist of a limited number of dependent and independent variables. However, many problem, for example in in HCI research, are not clearly defined or involve a large number of potentially influential factors. As a result, it is often very hard to construct a well-defined and testable hypothesis. This is especially true when studying an innovative interaction technique or a new user population and in the early development stage of a product.  

Experimental research also requires strict control of factors that may influence the dependent variables. That is, except the independent variables, any factor that may have an impact on the dependent variables, often called potential confounding variables, needs to be kept the same under different experiment conditions. This requirement can hardly be satisfied in many HCI / CTech studies. For example, when studying how older users and young users interact with computer-related devices, there are many factors besides age that are different between the two age groups, such as educational and knowledge background, computer experience, frequency of use, living conditions, and so on. If an experiment is conducted to study the two age groups, all those factors will all a significant impact on the observed results. This problem can be partially addressed in the data collection and data analysis stages. In the data collection stage, extra caution should be taken when there are known various influencing factors. Increasing the sample size may reduce the impact of the side factors. When recruiting participants, prescreening should be conducted to make the participants in different groups as homogeneous as possible.  

Lab-based experiments may not be a good representation of users' typical interaction behavior. It has been reported that participants may behave differently in lab-based experiments due to the stress of being observed, the different environment, or the rewards offered for participation. This phenomenon, called the *Hawthorne effect*, was documented around 60 years ago (Landsberger, 1958, as cited in [1]). The *Hawthorne effect* has by now be challenged and its effects were further refined. But still, we should keep this potential risk in mind and take precautions to avoid or alleviate the impact of the possible Hawthorne effect.




---
## Quantitative Research

Systematic investigation of observable phenomena via *statistical*, *mathematical*, or *computational techniques*.  

<br />
**The process of measurement is central.**


<br />
1. How to collect data?
2. How to analyse data?


???

Potential goal: To make conclusion from the collected data about the whole population.


---
## Quantitative Research

Systematic investigation of observable phenomena via *statistical*, *mathematical*, or *computational techniques*.  

<br />
**The process of measurement is central.**


<br />
1. How to collect data?
2. **How to analyse data?**


???

Potential goal: To make conclusion from the collected data about the whole population.





---
template:inverse


# Statistics in a Nutshell

---
## Statistics?


???
  

* Is what?

--

> The practice or science of collecting and analyzing numerical data in large quantities, especially for the purpose of **inferring proportions in a whole from those in a representative sample**.  
  
\- Apple Dictionary


???
  
We are currently living in the best and worst of times regarding data and its evaluation. 

* The importance of statistics and an awareness about potential problems when using statistics to make general statements is not new. Let's hear what some smart people said about this.

???
  

* Both the power and corruption of statistics are daily on display.
* Machine Learning is a statistical model

---
## Statistics

> The numbers have no way of speaking for themselves. We speak for them. We imbue them with meaning. 

\- [Nate Silver](https://en.wikipedia.org/wiki/Nate_Silver)
  
---
## Statistics

> Like dreams, statistics are a form of wish fulfillment.  
  
\- [Jean Baudrillard](https://en.wikipedia.org/wiki/Jean_Baudrillard)

---
## Statistics
> Figures don't lie, but liars figure.  
  
\- [Samuel Clemens](https://en.wikipedia.org/wiki/Mark_Twain) (alias Mark Twain, 1835 – 1910)


???
  

> If your experiment needs statistics, you ought to have done a better experiment.  
  
  
\- [Ernest Rutherford](https://de.wikipedia.org/wiki/Ernest_Rutherford) (1871 – 1937)


* In this lecture, however, we aim for an accurate display of information, not to obscure it. 
* Let's have a look first on what kind of data we might be working with.
* The process of transforming collected information or observations to a set of meaningful, cohesive categories.
* In the above example, the information on age is numerical and does not need to be coded. The information on gender, highest degree earned, and previous software experience needs to be coded so that statistical software can interpret the input.
* Coding can lead to a deeper understanding and the emergence of relationships, patterns, etc. When coding your data, the most important thing to remember is to ensure the coding is consistent.

---
## Statistics

.center[<img src="../02_scripts/img/statistic/garbage_01.png" alt="garbage_01" style="width:80%;">  .imgref[[[solutions.ait.ac.th]](http://solutions.ait.ac.th/garbage-in-garbage-out/)]]



---
## Statistics

???


* What is the difference between descriptive and inferential statistics?

--

Descriptive statistics 

* Summarizes data 
* Information is presented in a manageable form  


???
  
* Helps to describe and to organize data
* Descriptive statistics *summarizes* data and helps to describe and to organize data. Information is presented in a manageable form.  
* An example for descriptive statistics?

--

<br />

Inferential statistics 

* Anlayses data
* Draws conclusions about a population based on samples
    * Based on rigid requirements on the data and maths

???
  
* Generalizes and makes judgments
* Inferential statistics *draws conclusions* about a population based on samples and helps to analyze data. Inferential statistics is used to generalize and make judgments. For that there are rigid requirements on the data for the maths to work and to allow for such generalizations.


.center[<img src="../02_scripts/img/statistic/statistics_08.png" alt="statistics_08" style="width:50%;">]


---
.header[Descriptive Statistics]

## Graphs

.center[<img src="../02_scripts/img/statistic/statistics_22a.png" alt="statistics_22a" style="width:70%;">].imgref[[[oreilly]](https://www.oreilly.com/library/view/statistics-in-a/9781449361129/ch04.html)]

---
.header[Descriptive Statistics]

## Graphs

.center[<img src="../02_scripts/img/statistic/statistics_22b.png" alt="statistics_22b" style="width:70%;">].imgref[[[oreilly]](https://www.oreilly.com/library/view/statistics-in-a/9781449361129/ch04.html)]

---
.header[Descriptive Statistics]

## Graphs

.center[<img src="../02_scripts/img/statistic/statistics_22c.png" alt="statistics_22c" style="width:80%;">].imgref[[[oreilly]](https://www.oreilly.com/library/view/statistics-in-a/9781449361129/ch04.html)]


---
.header[Descriptive Statistics]

## Graphs

.center[<img src="../02_scripts/img/statistic/statistics_11.png" alt="statistics_11" style="width:70%;">].imgref[[[cellfish]](http://blog.cellfish.se/2014/08/lying-with-statistics.html)]




???
  

* *What might be the problem with the following bar charts?*
* Yes, exactly. One of the most common tricks used is to show small changes as huge by not using zero as the base in a diagram. 
* Looking at the Fox News graph it looks like as if there was a huge increase in the enrollment in Obamacare.  But the difference between the two numbers is only 33,000, which is in regard to the start value of 7,066,000 not that much. But as the graphic cuts off the bottom part of the graphs the increase is greatly exaggerated. Don't ask me what the intentions were, I always thought that Fox News was against Obamacare...

---
.header[Descriptive Statistics]

## Graphs

.center[<img src="../02_scripts/img/statistic/statistics_16.png" alt="statistics_16" style="width:70%;">].imgref[[[spiegel]](https://www.spiegel.de/politik/deutschland/rezo-video-die-youtube-angriffe-auf-die-cdu-im-spiegel-faktencheck-a-1268973.html) *Figure by Rezo*]




???
  

* In regard to the question of *wealth through inheritance*, Rezo showed the following figure:

---
.header[Descriptive Statistics]

## Graphs

.center[<img src="../02_scripts/img/statistic/statistics_17.png" alt="statistics_17" style="width:80%;">].imgref[[[spiegel]](https://www.spiegel.de/politik/deutschland/rezo-video-die-youtube-angriffe-auf-die-cdu-im-spiegel-faktencheck-a-1268973.html) *Original by Alvaredo et al./ Atkinson/ CC BY 4.0*]



???
  

* Not only did Rezo remove the graphs of the other countries in comparison, he also cut the timeline - the oldest and most evil move in regard to graph manipulations! In this case the manipulated figure implies that the historically exceptionally values between the 1960-90 (as it becomes clear from the original figure) were a normal phase. Spiegel calls this in its article about the fact-checking of Rezo's video *ein absolutes No-Go*. This is especially disappointing, as the Spiegel points out, as there is enough valid data to underline the point Rezo was overall trying to make.

---
.header[Graphs]

.center[<img src="../02_scripts/img/statistic/statistics_18.png" alt="statistics_18" style="width:44%;">].imgref[[[gregstevens]](http://gregstevens.com/2011/02/21/lying-with-statistics-101/)]

???
  

* Most people see in the graphic above a huge fall-off in the number of gun deaths after Stand Your Ground was passed. That is what the law was aiming for but that’s not what the graph shows. A quick look at the vertical axis reveals that the gun deaths are counted from top (0) to bottom (800). The highest peaks are the fewest gun deaths and the lowest ones are the most. A rise in the line, in other words, reveals a reduction in gun deaths. The graph below — flipped both horizontally and vertically — is more intuitive to most: a rising line reflects a rise in the number of gun deaths and a dropping a drop.

---
.header[Graphs]

.center[<img src="../02_scripts/img/statistic/statistics_19.png" alt="statistics_19" style="width:44%;">].imgref[[[gregstevens]](http://gregstevens.com/2011/02/21/lying-with-statistics-101/)]


---
.header[Descriptive Statistics]

## Frequency Distributions

Counts the number of times each score occurs.

.center[<img src="../02_scripts/img/statistic/statistics_25.png" alt="statistics_25" style="width:36%;">]

--

<br />

*How can the data be summed up and described with a single value?*  

---
.header[Descriptive Statistics | Frequency Distributions]

## Measures of Central Tendency

Calculate a *central tendency* and the *centric point* of a distribution. 


???
  

* Any examples?

---
.header[Descriptive Statistics | Frequency Distributions]

## Measures of Central Tendency

Calculate a *central tendency* and the *centric point* of a distribution. 
 


.center[<img src="../02_scripts/img/statistic/statistics_29a.png" alt="statistics_29a" style="width:70%;">]  
[[4]](https://docs.google.com/presentation/d/1cPWa6NqbEot8dBjVC7UKPjF72Q7myYjHqyBYS9HO_qg/edit#slide=id.g5137fefd78_1_189)

---
.header[Descriptive Statistics | Frequency Distributions]

## Measures of Central Tendency

Calculate a *central tendency* and the *centric point* of a distribution. 
 


.center[<img src="../02_scripts/img/statistic/statistics_29b.png" alt="statistics_29b" style="width:70%;">]  
[[4]](https://docs.google.com/presentation/d/1cPWa6NqbEot8dBjVC7UKPjF72Q7myYjHqyBYS9HO_qg/edit#slide=id.g5137fefd78_1_189)

---
.header[Descriptive Statistics | Frequency Distributions]

## Measures of Central Tendency

Calculate a *central tendency* and the *centric point* of a distribution. 
 


.center[<img src="../02_scripts/img/statistic/statistics_29c.png" alt="statistics_29c" style="width:70%;">]  
[[4]](https://docs.google.com/presentation/d/1cPWa6NqbEot8dBjVC7UKPjF72Q7myYjHqyBYS9HO_qg/edit#slide=id.g5137fefd78_1_189)

---
.header[Descriptive Statistics | Frequency Distributions]

## Measures of Central Tendency


.center[<img src="../02_scripts/img/statistic/statistics_38.png" alt="statistics_38" style="width:100%;">]  .imgref[[[taniapouli]](http://taniapouli.me/wp-content/uploads/2016/08/s2010_course.pdf)]


???
  

* What makes the two images different?


---
.header[Descriptive Statistics | Frequency Distributions]

## Measures of Central Tendency

![statistics_35](../02_scripts/img/statistic/statistics_35.png)  
.imgref[[[wiki]](https://en.wikipedia.org/wiki/Mean#/media/File:Comparison_mean_median_mode.svg)]


???
  

* If data is symmetrically distributed, the mean and median will be close, especially as n increases. If the data is skewed, mean, median and mode can differ greatly. Depending on our question, that might really matter... we will come back to this.

---
.header[Descriptive Statistics | Frequency Distributions]

## Measures of Central Tendency

> When is the mean not representative for a data set and why?


???
  

* For the data set of 5 5 5 5 5, the mean of 5 directly represents the actual values and is therefore a good measurement. However the single values of 6 8 4 1 6 differ quite strongly from the mean of 5, which therefore is not the best representation. This characteristic of how much the single values differ to the mean is reflected by the *variance* of a data set.


---
.header[Descriptive Statistics | Frequency Distributions]

## Measures of Spread



.center[<img src="../02_scripts/img/statistic/statistics_39.png" alt="statistics_39" style="width:70%;">]


???
  
To describe how data are situated around the central tendency, we use measures of data spread. 

* What makes them different, once again, is the *distribution* of values.

---
.header[Descriptive Statistics | Frequency Distributions]

## Measures of Spread


???
  

* Do you know one?

--

Mathematically, the variance is exactly the mean squared distance that values have to the mean. 

--

.left-even[<img src="../02_scripts/img/statistic/statistics_40.png" alt="statistics_40" style="width:90%;">]

--

.right-even[<img src="../02_scripts/img/statistic/variance_01.png" alt="variance_01" style="width:90%;">]


???
  

* What is the standard variation?
* As the formula for variance shows, the units of variance are the units of the observation *squared*, *x<sub>i</sub><sup>2</sup>* and in a variance's value can not be directly compared to the value range of the collected data. Hence, to get the metric back into units of the variable, we take the square root of the variance, which is called the *standard deviation σ*.
* A large value means that values are quite different from each other and that the data varies a lot. Hence, the mean is not representative for the data set. A small value means all data points are close to the mean with little variation. Hence, the mean is a valid representation of the data.
* Both *variance* and *standard deviation* measure the accuracy of the mean of data set and the variability of the data. Variance and standard deviation only differ in a scaling factor.

---
.header[Descriptive Statistics]

## Box and Whisker Plots

.center[<img src="../02_scripts/img/statistic/statistics_48.png" alt="statistics_48" style="width:20%;">]

???
  

* For an overview of the most important values, the core of a box and whisker plot, in short *boxplot*, looks as follows:
    * The box is bounded by the two inner quartiles of the data. This is called the *interquartile range* (IQR). Its middle line shows the median and the whiskers extend to the last observation within 1 step (usually 1.5 * IQR) from the end of the box.
* Quantiles are cut points that divide a sample of data into groups containing (as far as possible) equal numbers of observations. 

---
.header[Descriptive Statistics]

## Box and Whisker Plots

.center[<img src="../02_scripts/img/statistic/statistics_49.png" alt="statistics_49" style="width:30%;">]

???
  


.center[<img src="../02_scripts/img/statistic/statistics_50.png" alt="statistics_50" style="width:40%;">]




* Any observations beyond the whiskers are plotted as individual points.

---
.header[Descriptive Statistics |  Box and Whisker Plots]

.center[<img src="../02_scripts/img/statistic/statistics_53.png" alt="statistics_53" style="width:80%;">]


???
  

* What do the histogram plots, meaning the frequency distributions, of the following box and whisker plots look like?

---
.header[Descriptive Statistics |  Box and Whisker Plots]


.center[<img src="../02_scripts/img/statistic/statistics_54.png" alt="statistics_54" style="width:60%;">]


---
## Descriptive Statistics

--

In descriptive statistics we use

* measures of central tendency (mode, median, mean), and
* measures of spread (variance, standard deviation)

to describe and summarize data.

--

<br >

We usually report

* mean and standard deviation values in the accompanying text, and
* a summary of the data as box plots graphically.


???

Hence, we can summarize a <span style="color:blue">sample</span> (meaning a set of data) using its <span style="color:red">mean</span> and we can access the accuracy of that mean using the standard deviation.

![statistics_56](../02_scripts/img/statistic/statistics_56.png)  



But how well does one <span style="color:blue">sample</span> represent the <span style="color:darkorange">population</span>?

![statistics_57](../02_scripts/img/statistic/statistics_57.png)  


Probably not too well…  



We could take several <span style="color:blue">samples</span> from the same <span style="color:darkorange">population</span>, each with its own <span style="color:red">mean</span>.

.center[<img src="../02_scripts/img/statistic/statistics_58.png" alt="statistics_58" style="width:65%;">]



We could take several <span style="color:blue">samples</span> from the same <span style="color:darkorange">population</span>, each with its own <span style="color:red">mean</span>.

.center[<img src="../02_scripts/img/statistic/statistics_58.png" alt="statistics_58" style="width:45%;">]

Then, the question becomes whether the <span style="color:blue">sample distribution</span> is representative for the <span style="color:darkorange">population</span>?


???
  

* This question is answered with *inferential* statistics.


---
template:inverse

# Inferential Statistics

---
## Inferential Statistics

With inferential statistics you can reach to conclusions that extend beyond the immediate data alone but describe a population overall.


---
.header[Inferential Statistics]

## Hypothesis Testing

Determine the likelihood of a hypothesis about some parameter value to be true. 


???
  

* What is the approach here, key word: null hypothesis

--



* Null hypothesis H<sub>0</sub>: Assumes that there is no difference between two conditions
* Alternative hypothesis H<sub>A</sub>: Assumes significant differences between the two conditions



???
  

* Mü

--

We assume the null hypothesis to be true until proven otherwise.  

--

* Reject the null hypothesis (proven by the data)
* Fail to reject the null hypothesis


???
  

* The data is the evidence and we can make one of two decisions:


---
.header[Inferential Statistics]

## Hypothesis Testing

There are a variety of test available, each potentially including several steps.

???
  

* The first test, we need to have a look into is a test that uses a calculated probability to determine whether there is evidence to reject the null hypothesis, hence ideally showing that we have indeed observed an effect.
* There are two common tests for that, the [Critical Value approach](https://online.stat.psu.edu/statprogram/reviews/statistical-concepts/hypothesis-testing/critical-value-approach) and the [p-value approach](https://online.stat.psu.edu/statprogram/reviews/statistical-concepts/hypothesis-testing/p-value-approach). The p-value approach requires only one computation and most statistical software uses it. So let's have a look into that, starting with an example.

--
<br >

Common steps are:

--
* Test if there is an effect in comparison to random results

--
* Test how the data is distributed

--
* Test to quantify the effect

---
template:inverse

## Observed Effect or Random Results?

---
.header[Inferential Statistics]

## P-Value

--

> The probability of obtaining the results by chance.
  
--

It aims to prove statistical significance for a cause and effect!

???

* A small p-value means that the null hypothesis is very unlikely.
* A large p-value means that the  null hypothesis is very likely.

---
.header[Inferential Statistics | P-Value]

## Lady Tasting Tea

![tea](../02_scripts/img/statistic/tea.png)  
.imgref[[[wiki]](http://en.wikipedia.org/wiki/File:Milk_clouds_in_tea.jpeg)]


???
  

* [Dr. Muriel Bristol](https://en.wikipedia.org/wiki/Muriel_Bristol), a female colleague of mathematician [Sir Ronald Fisher](https://en.wikipedia.org/wiki/Ronald_Fisher) claimed to be able to tell whether the tea or the milk was added first to a cup. Fisher didn't believe her and claimed that she was just guessing.
* Fisher proposed to test Lady Bristol's ability with a within-group experiment design. 
* In a within-group design, also called *repeated-measure* design, participants are assigned to *all* conditions. For this type of grouping it is important to also randomize task orders.

---
.header[Inferential Statistics | P-Value]

## Lady Tasting Tea

* Eight cups, four of each variety, in random order
* Dr. Bristol tastes four cups and reports which she thought had milk added first

--

H<sub>0</sub>: The lady has no ability to distinguish the teas. Hence there is no causality, and we assume she is just guessing!


???
  

* Now, Fisher investigated - given the outcome of the experiment - what the probability would be of Dr. Bristol performing the way she did by just guessing?

---
.header[Inferential Statistics | P-Value]

## Lady Tasting Tea

> What are the probabilities for guessed outcomes?

--

* n = 8 total cups
* k = 4 cups chosen
  
Based on the [combination formula](https://en.wikipedia.org/wiki/Combination), these numbers lead to

$\binom{n}{k} = \frac{n (n-1)...(n-k+1)}{k (k-1)...1} = \frac{n!}{k!(n-k)!} = \frac{8 \cdot  7 \cdot 6 \cdot 5}{4 \cdot 3 \cdot 2 \cdot 1}  = 70$

possible answers.



---
.header[Inferential Statistics | P-Value]

## Lady Tasting Tea

| Correct Guesses | Combinations                  | Cases              | Probability                   |
| --------------- | ----------------------------- | ------------------ | ----------------------------- |
| 0               | oooo                          | $\binom {4}{0}=1$  | $\frac{1}{70}\approx 1.4\%$   |
| 1               | ooox,ooxo,oxoo,xooo           | $\binom {4}{1}=16$ | $\frac{16}{70}\approx 22.9\%$ |
| 2               | ooxx,oxxo,oxox,xxoo,xoox,xoxo | $\binom {4}{2}=36$ | $\frac{36}{70}\approx 51.4\%$ |
| 3               | xxxo,xxox,xoxx,oxxx           | $\binom {4}{3}=16$ | $\frac{16}{70}\approx 22.9\%$ |
| 4               | xxxx                          | $\binom {4}{4}=1$  | $\frac{1}{70}\approx 1.4\%$   |

???
  
* Cases: ways to choose k correct cups (while there are 4 cups that truly had milk first)
    * ways to choose  0 milk-first
    * ways to choose  1 milk-first
    * ways to choose all 2 milk-first
    * ways to choose all 3 milk-first
    * ways to choose all 4 milk-first
    * we calculate \binom{4}{k}



* The frequencies of the possible numbers of successes, given in the final column of this table, are derived as follows. For 0 successes, there is clearly only one set of four choices (namely, choosing all four incorrect cups) giving this result. For one success and three failures, there are four correct cups of which one is selected, which by the combination formula can occur in {\displaystyle {\binom {4}{1))=4} different ways (as shown in column 2, with x denoting a correct cup that is chosen and o denoting a correct cup that is not chosen); and independently of that, there are four incorrect cups of which three are selected, which can occur in {\displaystyle {\binom {4}{3))=4} ways (as shown in the second column, this time with x interpreted as an incorrect cup which is not chosen, and o indicating an incorrect cup which is chosen). Thus a selection of any one correct cup and any three incorrect cups can occur in any of 4×4 = 16 ways. The frequencies of the other possible numbers of successes are calculated correspondingly. Thus the number of successes is distributed according to the hypergeometric distribution. The distribution of combinations for making k selections out of the 2k available selections corresponds to the kth row of Pascal's triangle, such that each integer in the row is squared. In this case, k=4 because 4 teacups are selected from the 8 available teacups.

The critical region for rejection of the null of no ability to distinguish was the single case of 4 successes of 4 possible, based on the conventional probability criterion < 5%. This is the critical region because under the null of no ability to distinguish, 4 successes has 1 chance out of 70 (≈ 1.4% < 5%) of occurring, whereas at least 3 of 4 successes has a probability of (16+1)/70 (≈ 24.3% > 5%). 

--
<br >

If the lady is guessing, there is only a *1.4%* chance that she will get all cups correct.


---
.header[Inferential Statistics | P-Value]

## Lady Tasting Tea

Fisher decided to accept that Dr. Bristol's has indeed an ability to taste the difference if she identified all four cups correctly. 


???
  

* Fisher argued that the probability for guessing this case is just too low. Hence, only with the identification of all four cups, Fisher were willing to reject the null hypothesis that Dr. Bristol is guessing.

Dr. Bristol identified all four cups of tea successfully 😀! Apparently, pouring hot tea into cold milk makes the milk curdle, but not so when pouring cold milk into hot tea.

--


<br />
This experiment is an example of the p-value approach to hypothesis testing.  


???
  
  
It uses a calculated probability p to determine whether there is evidence to reject the null hypothesis.  

--

> If the calculated **probability for randomness** is below a certain threshold, also called the *significance level α*, we assume cause and effect in our data. 



---
.header[Inferential Statistics]

## P-Value


Typical alpha levels of significance are

* p < .05 (Fisher-Criterion)
* p < .01
* p < .001

Which α to chose also depends on the discipline.  

???
  
We can reject H<sub>0</sub> if the computed p‐value is ≤ α. 

* The Fisher-Criterion is the most commonly chosen one. It means th experiment results hav the same probability of five correct guesses in a row:

| Number of correct guesses | Probability for consecutive correct guessing |
| ------------------------- | -------------------------------------------- |
| 1x                        | 0.5                                          |
| 2x                        | 0.25                                         |
| 3x                        | 0.125                                        |
| 4x                        | 0.0625                                       |
| 5x                        | 0.03125                                      |

--

> This shows that what we accept as *proven effect* also simply depends on somewhat arbitrary standards a research community decided on.

---
template:inverse

# Data Distribution

???
  

## Parametric Statistics

The factor that we want to investigate is called *parameter* and is some *numerical characteristics* of interest.  

Is there any difference between conditions or groups?
  
> Compare the arithmetic means over the collection of samples.


Usually, we want to find out whether there is any difference between conditions or groups and do so by comparing the arithmetic means over their sample collection.  


Then, the mean μ is the parameter. 


* e.g. patients that took a new drug vs. patients that didn't, etc. 
* Keep in mind that, we don't — or can't — know the real value of a population parameter; we only ever estimate it with applying statistics!

---
.header[Inferential Statistics]

## Parametric Statistics

For parametric tests to work, we have to assume some underlying statistical distributions in the data:

--
1. Normal distribution

--
2. Homogeneity of variance


???
  

* Shows that the mean is representative for the data set.
* Homogeneity of variance is given when all comparison groups have about the same variance, meaning that the data spread is similar enough to make meaningful comparisons between the groups. 


---
.header[Inferential Statistic | Parametric Statistics]

## Normal Distribution

> What is a normal distribution? Why is it important to test whether a data sample is normally distributed?

--

Many statistical computations only work when the data is roughly shaped as such a *bell curve* or *Gaussian curve*.

???
  

* The dependent variable must follow a normal distribution in the population.
* A normal distribution is the most important and widely used distribution in statistics.

---
.header[Inferential Statistic | Parametric Statistics]

## Normal Distribution

.center[<img src="../02_scripts/img/statistic/statistics_62.png" alt="statistics_62" style="width:85%;">]  
[[socialresearchmethods]](http://www.socialresearchmethods.net/kb/statinf.php)  


???
  

* The sampled data is roughly shaped as a bell curve or Gaussian curve. In other words, it is symmetric around its mean, median and mode (which are all equal). The area under the normal curve is equal to 1.0.
* It is important because we have to choose the right significance test and it can only be a parametric tests when the data is normally distributed. The significance test is necessary, so we know wether two or more samples are representative for the population.
* Their importance is partly due to the central limit theorem. It states that, under some conditions, the average of many samples (observations) of a random variable with finite mean and variance is itself a random variable—whose distribution converges to a normal distribution as the number of samples increases. Therefore, physical quantities that are expected to be the sum of many independent processes, such as measurement errors, often have distributions that are nearly normal.[4]
* In probability theory, the central limit theorem (CLT) establishes that, in many situations, for identically distributed independent samples, the standardized sample mean tends towards the standard normal distribution even if the original variables themselves are not normally distributed. 


---
.header[Inferential Statistics]

## Parametric Statistics

For parametric tests to work, we have to assume some underlying statistical distributions in the data:

* Normal distribution
    * [W/S test](http://article.sciencepublishinggroup.com/pdf/10.11648.j.ajtas.s.2017060501.19.pdf)
    * [Jarque-Bera test](https://en.wikipedia.org/wiki/Jarque%E2%80%93Bera_test)
    * [Shapiro-Wilks test](https://en.wikipedia.org/wiki/Shapiro%E2%80%93Wilk_test)
    * [Kolmogorov-Smirnov test](https://en.wikipedia.org/wiki/Kolmogorov%E2%80%93Smirnov_test)
    * [D’Agostino test](https://en.wikipedia.org/wiki/D%27Agostino%27s_K-squared_test)
* Homogeneity of variance
    * [Levene’s test](https://en.wikipedia.org/wiki/Levene%27s_test)


---
template:inverse

# Quantification of the Effect

---
.header[Inferential Statistics]

## Parametric Statistics

> How can we compare the means of two independent groups?

---
.header[Inferential Statistics | Parametric Statistics]

## t-Test

The two-sample test analyses two sample means 𝜇1 and 𝜇2.  


???
  

* Under the assumptions that the data is normal distributed and the homogeneity of variance is given, we then can use, for example a t-test for comparing two populations. The two-sample test analyses two population means 𝜇1 and 𝜇2. The t stands for the difference between means in relation to the variation in the data. It describes the degree to which those means differ by chance alone.

--

> Is there a statistical difference between the means of the two samples?
  
---
.header[Inferential Statistics | Parametric Statistics]

## t-Test
  
 
* One-sample: Compare one group to a give value

???
  

* Du möchtest herausfinden, ob deine Schokoladen-Riegel wirklich 300 Gramm im Durchschnitt wiegen, wie es auf der Packung steht. Um das zu testen, wiegst du 40 Riegel ab und vergleichst das tatsächliche Gewicht mit dem Gewicht, das sie haben sollten (300 Gramm).
* https://www.scribbr.de/statistik/t-test/

--
* Unpaired: Compare two group means from independent groups

???
  

* Du möchtest wissen, ob sich die durchschnittliche Größe von Männern von jener von Frauen unterscheidet.
* An unpaired t-test (also known as an independent t-test) compares the averages of two independent or unrelated groups to determine if there is a significant difference between the two. 

--
* Paired: Compare two group means from groups that are dependent


???


* Du misst die Größe derselben Person im Jahr 2015 und im Jahr 2018. Die dabei ermittelten Werte sind klar voneinander abhängig. Du verwendest einen abhängigen t-Test.
* A paired t-test (also known as a dependent or correlated t-test) compares the averages and standard deviations of two related groups to determine if there is a significant difference between the two groups. 
* Here, the two groups are paired or connected, e.g. participants before and after a treatment or a repeated-measure grouping, where all participants do all tasks.
* https://askanydifference.com/difference-between-t-test-and-p-value/

---
.header[Inferential Statistics | Parametric Statistics]

## t-Test

Most statistical software include p-value and t-test functions. It takes as input the raw data.

---
.header[Inferential Statistics | Parametric Statistics]

## t-Test

.left-even[<img src="../02_scripts/img/statistic/google_01.png" alt="google_01" style="width:45%;">]

.footnote[[Flandorfer, P. 2023. [Den T-Test verstehen und interpretieren mit Beispiel](https://www.scribbr.de/statistik/t-test/). Scribbr]]

???
  

* https://docs.google.com/spreadsheets/d/12dxoiJHh2vN-xhNrPD68xcMYDrZ8A_GTEbpZNP9Pxgc/edit?usp=sharing
* https://www.scribbr.de/statistik/t-test/
* https://docs.google.com/spreadsheets/d/1I3OARNrSNQ0lZqV4hnNj4yJZmBAaoB_jpgeN9-cKLog/edit#gid=997673686
* The independent t-test returns a value for the difference in means of the two conditions, t, with larger t values suggesting higher probability of the null hypothesis being false.  
* In other words, the higher the t value, the more likely the two means are different.

--
.right-even[<img src="../02_scripts/img/statistic/google_02.png" alt="google_02" style="width:100%;">]


???
  

* we get a value for the difference in means of ~-4.3, meaning there is an estimated change of 4.3%.


???
  

* Suppose you want to investigate the effect of caffeine on muscle metabolism.
* You work with 18 male volunteers and randomly assign them to two groups. In one group all men take a capsule containing pure caffeine, the men in the other group receive a placebo capsule. 
* The two groups are independent from each other and the independent-samples t test is appropriate for data analysis.


H<sub>0</sub>: The mean of the caffeine treatment equals the mean of the placebo  
H<sub>A</sub>: The mean of caffeine treatment is not equal to the mean of the placebo treatment.  


.center[<img src="../02_scripts/img/statistic/statistics_75.png" alt="statistics_75" style="width:100%;">]



* Now, all men underwent arm exercise tests. During each exercise the subject's respiratory exchange ratio (RER) was measured. RER is the ratio of CO2 produced to O2 consumed and is an indicator of whether energy is being obtained from carbohydrates or fats.



The independent t-test returns a value for the difference in means of the two conditions, t, with larger t values suggesting higher probability of the null hypothesis being false.  


In other words, the higher the t value, the more likely the two means are different.


---
.header[Inferential Statistics | Parametric Statistics]

## t-Test

Exemplary summary in a paper:

> The average height of women (M = 166.3; SD = 10.03) is lower than that of men (M = 183.1; SD = 11.21) with a significant difference of t(28) = -4.34 and p < 0.001.


.footnote[[Flandorfer, P. 2023. [Den T-Test verstehen und interpretieren mit Beispiel](https://www.scribbr.de/statistik/t-test/). Scribbr]]

???
  

* Bei unabhängigen Stichproben (beim Zweistichproben-t-Test) solltest du dabei auf jeden Fall folgende Parameter angeben:
* Mittelwert und Standardabweichung für beide Gruppen,
* t-Wert mit der Anzahl der Freiheitsgrade und
* Signifikanz (Sig.) des t-Tests.


---
.header[Inferential Statistics | Parametric Statistics]

## t-Test


.center[<img src="../02_scripts/img/statistic/statistics_76.png" alt="statistics_76" style="width:90%;">]  [[15]](http://learntech.uwe.ac.uk/da/Default.aspx?pageid=1438)  

???
  

* If we run an independent-samples t-test using the data set above, we get a value for the difference in means of ~6.4, meaning there is an estimated change of 6.4%.
  
Statistical software generates a summary table for the results, containing both the t-test results, additional test results that examine the data distribution and a p-value. 


---
.header[Inferential Statistics | Parametric Statistics]

## t-Test


.center[<img src="../02_scripts/img/statistic/statistics_77.png" alt="statistics_77" style="width:90%;">]  [[15]](http://learntech.uwe.ac.uk/da/Default.aspx?pageid=1438)  


*But* the p-value is 0.063 and, therefore, the difference between the two means is not statistically significantly different with a 5% level of significance.


???
  

* In this case, we would need to report as result that even though there is a change of 6.4% observed, it is insufficient evidence (p = 0.063) to generally conclude that caffeine does change the mean RER.
* The specific values returned from a software computation and how to use and interpret the given values is highly dependent from the specific package. When needed, decide on a software early on and make sure that you know how to input the data (layout, format?) and what to expect as output. This can take some time and effort, so please make sure to have enough time and brain power left to do this
* Again, keep in mind, that the t-test is only applicable for the comparison of two groups. If you want to test three or more parameter, you have to use a different statistical test.

---
.header[Inferential Statistics | Parametric Statistics]

## One-way ANOVA

???
  

* NOVA steht für Varianzanalyse (engl. Analysis of Variance) und wird verwendet um die Mittelwerte von mehr als 2 Gruppen zu vergleichen.
* Sie ist eine Erweiterung des t-Tests, der die Mittelwerte von maximal 2 Gruppen vergleicht.

* Many studies involve three or more conditions that need to be compared. Due to variances in the data, you should not directly compare the means of the multiple conditions and claim that a difference exists as long as the means are different. Instead, you have to use statistical significance tests to evaluate the variances that can be explained by the independent variables and the variances that cannot be explained by them. The significance test will suggest the probability of the observed difference occurring by chance. If the probability that the difference occurs by chance is fairly low (e.g., less than 5%), we can claim with high confidence that the observed difference is due to the difference in the controlled independent variables.

--
The comparison of more than *two* groups, based on *one* factor.

--
* The difference in height between soccer player, gymnasts, and volleyball players.
    * Factor: height in cm
    * Groups: soccer, gymnastics, and volleyball
--
* The the productivity of three or more employees based on working hours.
    * Factor: the productivity in working hours
    * Groups: three or more employees


???
  

* The one way Analysis of Variance (ANOVA) compares more than *two* groups, based on *one* factor. This means that there is only one independent variable.
* Collected soil uranium concentrations at three locations: Site A, Site B, and Site C.
    * Factor: the uranium concentration
    * Groups: three locations
* The hardness of four blends of paint
    * Factor: the hardness of paint
    * Groups: four blends of paint

---
.header[Inferential Statistics | Parametric Statistics]

## One-way ANOVA

Again, keep in mind that there are various requirements for the data:

* Normally distributed
* Homogeneity of variance
* Samples are independent

--

Then we test the following hypotheses:

H<sub>0</sub>: All means are equal.  
H<sub>A</sub>: *At least one* of the means is different from the others.

---
.header[Inferential Statistics | Parametric Statistics]

## One-way ANOVA


.center[<img src="../02_scripts/img/statistic/statistics_69.png" alt="statistics_69" style="width:70%;">]  


???
  

* https://docs.google.com/spreadsheets/d/1I3OARNrSNQ0lZqV4hnNj4yJZmBAaoB_jpgeN9-cKLog/edit#gid=1929990740
* The ANOVA test is based on the assumption that if the between group variance is much larger than the within group variance, then it seems more likely that the groups are different.  

---
.header[Inferential Statistics | Parametric Statistics]

## Two-way ANOVA

The [two way ANOVA](https://en.wikipedia.org/wiki/Two-way_analysis_of_variance) can compare more than two groups, based on *two* factors. 
  
This means that there are two independent variables, e.g., compare the employee productivity based on the working hours *and* some quality measure of their results.


???
  

* We will not get into this any further. If you need this test at some point, please investigate it properly.

---
.header[Inferential Statistics]

## Non-Parametric Statistics

--

Non-parametric methods make fewer assumptions about the data. They are also called *assumption-free* tests.


???
  

* E.g. if your data is not distributed normally.
* Although nonparametric tests are also called *assumption-free* tests, it should be noted that they are not actually free of assumptions. For example, the [Chi-squared test](https://en.wikipedia.org/wiki/Chi-squared_test), one of the most commonly used nonparametric tests, has specific requirements on the sample size and independence of data points. 
* Non-parametric analysis sacrifices the power to use all available information to reject a false null hypothesis in exchange for less strict assumptions about the data.
* Another important message to note about nonparametric analysis is that information in the data can be lost when the data tested are actually interval or ratio. 
* The reason is that the nonparametric analysis collapses the data into ranks so all that matters is the order of the data while the distance information between the data points is lost. 

---
## Choosing a Statistical Test

--

Planing the statistical analysis should be an integral part of designing a study!

--

You should answer the following questions in advance:

* What kind of data?
* How many independent variables? 
* Independent measure or repeated measure design?
* Is the data parametric or non-parametric?


???
  

* Depending on what you already know, you can easily find guidelines on which statistical test to chose, such as the following for example.

---
## Choosing a Statistical Test

.center[<img src="../02_scripts/img/statistic/statistics_80.png" alt="statistics_80" style="width:64%;">]  

---

## Inferential Statistics

--

> Statistical analysis is a powerful tool that helps us find interesting patterns and differences in the data as well as identify relationships between variables, enabling us to make assumptions about a population. 
  
--
  
Before running significance tests, the data needs to be cleaned up, coded, and appropriately organized to meet the needs of the specific statistical software package. 


???
  

* The nature of the data collected and the design of the study determine the appropriate significance test that should be used.  
  
---

## Inferential Statistics
  

> Do not underestimate the effort you need to put into using methods of inferential statistics!


???
  

* If the data is normally distributed, parametric tests, such as a t-test or an ANOVA, are appropriate. When the normal distribution requirements are not met, nonparametric tests should be considered.  
* This section about inferential statistic left you probably with many open questions and you might now feel a bit scared about inferential statistic. Good 🙃... What I mean with this is that you must be aware of the complexity of inferential statistics and that you need to invest time and effort to use it properly. 
* The topic is, however, absolutely worth it, as it might lead to meaningful results. And ultimately, with this chapter as a starting point, giving you all the important keywords, and with a somewhat structured approach, inferential statistic is quite conquerable after all! 💪🏼







---
template:inverse

## The End

# 👋🏻

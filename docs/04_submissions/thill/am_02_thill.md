---
layout: default
title: Session 01
nav_exclude: true
---

## **Task 01.01 - Question Selection**

### 1st Topic - Fallacies

- What are the linguistic and contextual challenges in automating fallacy detection in debates and what are current solutions?
- How can chain-of-thought reasoning prompts improve fallacy classification accuracy in generative models?
- To what extent can real-time fallacy detection enhance participants’ argumentative clarity and accountability in live discussions?
- How does real-time fallacy detection differ in accuracy between structured debates and open-ended conversations?
- Which types of informal fallacies are most frequently misclassified by language models in structured argumentation?
- How can interactive installations use argument quality as an input for shaping real-time generative media?
    - Example: In a debate-performance, the more logically flawed the discourse, the more chaotic the visual/audio output becomes — a performative data-art critique.
- How does prompt phrasing influence fallacy detection accuracy in large language models?
- How does the inclusion of natural language explanations affect user trust in automated fallacy detection systems?
    - Tool: Build a front-end where users can see not just the fallacy label, but an auto-generated explanation.
    - Test: Basic user study (5–10 people) on perceived clarity, helpfulness, trust.
- Can a multi-model system generate Socratic-style feedback to guide users toward recognising their own fallacies?
    - ML chain: Use one model to classify the fallacy, another to generate a question like “What assumption is being made here?”
Goal: Create a lightweight tutoring agent for critical thinking.

### 2nd Topic - News

- How does the source diversity of summarised news content affect perceived trust and informativeness?
    - What you’d do: Compare multi-source summaries (from varied political leanings or geographies) vs. single-source summaries.
    - Evaluation: Small user study where participants rate trust and clarity.
- How does hyperlink integration into summarised news affect user verification behaviour and trust?
    - Usability study: Track whether users actually follow links, and whether having clickable sources increases perceived transparency.
- Does letting users pre-define preferred tones (e.g. neutral, optimistic, critical) influence engagement with summarised content?
    - LangChain angle: Add prompt-engineering layer to alter summary tone.
    - Test: Simple feedback loop or A/B preference test.
- How reliable is summarisation consistency when rerunning the same chain with identical inputs on different days (model drift)?

### 3rd Topic - Taste

- Examples of how people justify their artistic preferences when confronted with opposing opinions in a structured discussion?
- What rhetorical strategies are used to legitimise or delegitimise taste in comment sections of music review platforms?
- How is musical taste used to signal identity and status in online subcultures?
    - Focus on a few Reddit threads, Discord servers, or YouTube communities.
    - Easy to frame around signalling theory and cultural capital.
- Analysing examples of how the concept of ‘bad taste’ has been weaponised in avant-garde artistic movements?
- What role does institutional validation (e.g. festivals, residencies, journals) play in defining aesthetic legitimacy today?
    - A conceptual paper with grounded examples (could involve minor interviews or just reviewing application calls and press language).
- Does perceived effort correlate with perceived artistic value in generative artworks?
- What is the listener's perceptual response to ‘imperfect’ versus ‘clean’ versions of the same digital music track?
    - A simple A/B test survey could measure perceived emotional impact, “authenticity”, or warmth.
- How does the desire for imperfection reflect a resistance to algorithmic standardisation in digital creative culture?
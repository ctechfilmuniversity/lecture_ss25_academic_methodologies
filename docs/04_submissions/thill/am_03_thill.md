---
layout: default
title: Session 01
nav_exclude: true
---

## Task 03.02

### Final Description

This paper investigates whether chain-of-thought (CoT) reasoning prompts can improve the accuracy with which generative language models classify logical fallacies. Logical fallacies, which are recurring patterns of flawed reasoning, remain difficult to identify computationally due to their subtle, context-dependent nature. Building on recent developments in prompt engineering, this study examines whether CoT prompting, which encourages models to articulate their intermediate reasoning steps, can improve the accuracy and explainability of identifying logical fallacies compared to direct classification methods. A small desktop application will be developed using LangChain to coordinate multiple models and analyse debate excerpts sourced from online platforms. To strengthen the evaluation, existing benchmarks and annotated datasets — potentially including the MAFALDA benchmark for multi-task fallacy detection — will be considered for comparative testing. Additionally, relevant literature on computational argumentation and CoT prompting will be reviewed to contextualise and inform the approach. The aim is to further our understanding of how structured reasoning can be used to improve the accuracy and interpretability of automated assessments of human discourse.

### Prompts used

- [→ See ChatGPT-Chat](https://chatgpt.com/share/6830a4cb-b35c-8007-8506-79b89fb54d40)
- I used [DeepL Write](https://www.deepl.com/de/write) to refine the text




---
layout: default
title: Session 03
nav_exclude: true
---

## Session 03 - Research

## Your Paper

### Task 03.02 - Final Description

#### preliminary research question

How can dimensionality reduction techniques like PCA, t-SNE, and UMAP be configured and used to generate a training dataset that maps encyclopedic texts to semantically coherent spatial coordinates?

#### description

Dimensionality reduction (DR) plays an integral role in most fields affiliated with information processing, e.g., data compression and machine learning[^1]. Beyond that, it serves as a powerful tool to visually understand and analyze higher-dimensional data. These techniques can be applied to a large existing dataset to generate a new training set, where each original data point is mapped to a corresponding spatial coordinate for use in supervised predictive modeling tasks.

In this paper, the focus lies on an encyclopedic text corpus (like the Wikipedia dataset[^2]) and on representing overall thematic similarity as spatial proximity between entries. Based on selected literature that investigates the use of techniques such as PCA, t-SNE, or UMAP for the visualization of semantic relationships within data, the goal is to derive a practical framework to generate such a dataset. Ideally, aspects such as data cleaning and quality measurements would also be considered.

---

[^1]: Ghodsi, A. (2006). Dimensionality Reduction A Short Tutorial. https://www.researchgate.net/publication/249907764_Dimensionality_Reduction_A_Short_Tutorial
[^2]: Wikimedia/wikipedia Â· Datasets at Hugging Face. (2025, April 18). https://huggingface.co/datasets/wikimedia/wikipedia

#### prompts

literature search: https://chatgpt.com/share/6830d1cb-6190-8008-ba48-f658b836a09d
grammar refinement: https://chatgpt.com/share/6830dea9-b104-8008-b7dc-8e529b4a754b

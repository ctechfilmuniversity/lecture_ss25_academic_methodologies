---
layout: default
title: Session 03
nav_exclude: true
---

## Session 03 - Research

## Your Paper

### Task 03.02 - Final Description

#### preliminary research question

How can dimensionality reduction techniques like PCA, t-SNE, and UMAP be configured and used to generate a training dataset that maps encyclopedic texts to semantically coherent spatial coordinates?

#### description

Dimensionality reduction (DR) plays an integral role in most fields affiliated with information processing, eg. data compression and machine learning[^1]. Beyond that, it serves as a powerful tool to understand and analyze higher-dimensional data visually. Applying these techniques to a large existing dataset can be used to capture the resulting mapping of datapoint and spatial coordinate as a new training dataset for supervised predictive modeling tasks.

With this paper the focus lies on an encyclopedic text corpus (like the wikipedia dataset[^2]) and overall thematic similarity as spatial proximity between entries. Based on selected literature that investigates the usage of techniques like PCA, t-SNE or UMAP for the visualization of semantic relationships within data, the goal is to derive a practical framework to generate such a dataset. Optimally, aspects like data cleaning and quality measurements would also be considered.

---

[^1]: Ghodsi, A. (2006). Dimensionality Reduction A Short Tutorial. https://www.researchgate.net/publication/249907764_Dimensionality_Reduction_A_Short_Tutorial
[^2]: Wikimedia/wikipedia Â· Datasets at Hugging Face. (2025, April 18). https://huggingface.co/datasets/wikimedia/wikipedia

#### prompts

https://chatgpt.com/share/6830d1cb-6190-8008-ba48-f658b836a09d
